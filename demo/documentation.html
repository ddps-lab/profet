<!DOCTYPE html>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="./index.js"></script>
<link rel="stylesheet" type="text/css" href="./index.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
<link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.indigo-pink.min.css">
<link href='http://fonts.googleapis.com/css?family=Roboto:400,100,100italic,300,300italic,400italic,500,500italic,700,700italic,900italic,900' rel='stylesheet' type='text/css'>
<script defer src="https://code.getmdl.io/1.3.0/material.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.5.0/Chart.min.js"></script>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>PROFET</title>
</head>
<body>
<div class="main">
    <div class="title">
        <h2><a href="./index.html">PROFET</a></h2>
        <div id="nav-bar">
            <Button onclick="navBarClick(this)" name="index">About</Button>
            <Button onclick="navBarClick(this)" name="documentation">Documentation</Button>
            <Button onclick="navBarClick(this)" name="demo">Demo</Button>
            <Button onclick="navBarClick(this)" name="contact">Contact</Button>
        </div>
    </div>
    <div class="page-body">
        <header>
            <h3 class="page-title">Documentation</h3>
        </header>
            <h5>Understand the PROFET architecture</h5>
            <img src="./src/image01.png"/>
            <ul class="bulleted-list">
                <li>USER
                    <p>A client that wants to estimate the training time of a custom CNN implementation runs the code on a given, randomly chosen anchor instance with profiling enabled and submits the profiled output to the predictive model in json format.</p>
                </li>
            </ul>
            <ul class="bulleted-list">
                <li>PROFET SYSTEM
                    <p><strong>PROFET: PROFiling-based CNN Training Latency prohET for GPU Cloud Instances</strong></p>
                    <p>Using profiling results that do not involve internal model architectures but contain high-level categorical latencies of different tasks, PROFET predicts training times for different cases, such as different instance types, batch sizes, and input image pixel sizes.</p>
                    <p><strong>Feature Engineering with NLP clustering</strong></p>
                    <p>Group similar operations to improve the prediction accuracy of models that use infrequently used operation names via the operation clustering module.</p>
                    <p><strong>Anchor Instance Prediction &amp; Input/Batch Size Prediction</strong></p>
                    <p>As the first method in the latency prediction module, PROFET predicts the expected training time of a random model on the target instance based on the profiling results of the anchor instance. Another way to estimate PROFET uses the predicted latency on the target instance type to linearly predict training latency times for unique batch and input image pixel sizes on the target instance type. Unlike instance types, the impact of batch and input image pixel size changes can be expressed using the ratio of the numerical difference.</p>
                    <p>
                    </p>
                </li>
            </ul>
            <h5>
                <strong>Try PROFET out for the first time</strong>
            </h5>
            <img src="./src/image02.png"/>
            <ul class="bulleted-list">
                <li>Web Demo <p>
                    <strong>PROFET Anchor Prediction</strong>
                    <div class="indented">
                        <p>step 1. Select the achor instance to be profiled.</p>
                        <p>step 2. Upload the profiled result in json format.</p>
                        <p>step 3. <strong>Optionally</strong> enter the batch latency of the anchor instance.
                        <div class="indented">
                            <p>If you enter batch latency of anchor instance as input, the result is derived using the median value of the three learning models used for final latency prediction. Conversely, if batch latency is not input, the latency prediction result is specified as the average value of the two learning models. Therefore, it is optional to proceed.</p>
                        </div>
                    </div>
                    <p><strong>PROFET Scaler Prediction</strong>
                    <div class="indented">
                        <p>Using two latency values, we can predict the latency of different batch sizes linearly.</p>
                        <p>step 1. Select the target instance for which you want results.</p>
                        <p>step 2. Choose one of batchsize and dataset size if you want to see latency results with different values.</p>
                        <p>step 3.
                        <div class="indented">
                            <p>3-1 . Based on batchsize, latency at batch size 16 becomes the Latency Min value, and latency at batch size 256 becomes Latency Max.</p>
                            <p>3-2. Likewise, based on the dataset size , a dataset of 32 when the latency is Latency Min , Latency Max value is a value when 256 days dataset size.</p>
                        </div>
                    </div>
                </li>
            </ul>
            <ul class="bulleted-list">
                <li>Docker <h5><strong>Run Docker</strong></h5>
                    <p>First, install the required docker and pull the image that matches the tensorflow version.</p>
                    <p>Run the tensorflow 2.5.0 docker using the pulled image.</p>
                    <pre class="code code-wrap">
<code>sudo snap install docker
sudo docker pull tensorflow/tensorflow:2.5.0
sudo docker run -it tensorflow/tensorflow:2.5.0 bash</code>
                    </pre>
                    <h5><strong>Setting</strong></h5>
                    <pre class="code code-wrap">
<code>apt-get update
apt-get install git -y
cd home
git clone https://github.com/anonymous-profet/profet.git
cd profet
pip install -r requirements.txt</code>
                    </pre>
                    <p>Update information about available packages and their versions through apt-get update.</p>
                    <p>And then , install git and move the path to home.</p>
                    <p>After git clone of PROFET, access the profet folder and install the necessary packages.</p>
                    <h5><strong>Data Preprocessing</strong></h5>
                    <pre class="code code-wrap">
<code>cd data
python anchor_preprocessing.py</code>
                    </pre>
                    <p>First, execute anchor_preprocessing.py in the data folder to preprocess the data that matches the batch latency of the target instance to the profiling feature of the anchor instance. For detailed preprocessing method, refer to figure 6.</p>
                    <h5><strong>Profet Inference</strong></h5>
                    <p>In all processes suggested, the explanation specified in No.1 is for PROFET Anchor Prediction, and the process in No.2 is for PROFET Scaler Prediction.</p>
                    <pre class="code code-wrap">
<code>cd ../profet
# train
python train_anchor_model.py --anchor_instance g3s.xlarge
python train_scaler_model.py

#inference
python anchor_prediction.py --filename &#x27;vgg16_224ds_16bs_test.json&#x27; --anchor_instance &#x27;g3s.xlarge&#x27; --anchor_latency 333
python scaler_prediction.py --target_instance g3s.xlarge --latency_min 10 --latency_max 100 --size_pred 128 --batch_or_dataset batchsize</code>
                    </pre>
                    <ul class="bulleted-list">
                        <li>train <ol type="1" class="numbered-list" start="1">
                            <li>Run train_anchor_model.py in the profet folder to train the model. At this time, enter anchor_instance as a parameter to specify which instance to learn based on.</li>
                        </ol>
                            <ol type="1" class="numbered-list" start="2">
                                <li>train_scaler_model.py also proceeds with the model training process.</li>
                            </ol>
                        </li>
                    </ul>
                    <p>
                    </p>
                    <ul class="bulleted-list">
                        <li>inference <ol type="1" class="numbered-list" start="1">
                            <li>When executing anchor_prediction.py file , you can check the prediction result by entering filename , anchor instance , and anchor latency parameters.
                                <p>( * Unlike the web demo when running anchor_prediction.py , input for anchor latency is a required parameter. )</p>
                                <p>After execution, the result is output as &quot; anchor instance - target instance - predicted latency[ms] &quot; .</p>
                            </li>
                        </ol>
                            <ol type="1" class="numbered-list" start="2">
                                <li>When running scaler_prediction.py, first input the <strong>target_instance</strong> type and determines whether to check the result when the batchsize changes or when the dataset changes as <strong>&#x27;batch_or_dataset&#x27;</strong>.
                                    <p>2-1. If you want to check the result based on batchsize ,</p>
                                    <div class="indented">
                                        <p><strong>&#x27;latency_min&#x27;</strong> as latency when batchsize is 16, and &#x27;<strong>latency_max&#x27;</strong> as latency when batchsize is 64 . </p>
                                        <p>And input the latency of the batchsize size you want to know as &#x27;<strong>size_pred</strong>&#x27;.</p>
                                    </div>
                                    <p>2-2. If you want to check the result based on dataset size  ,</p>
                                    <div class="indented">
                                        <p>
                                            <strong>&#x27;latency_min&#x27;</strong> as latency when dataset size is 32, and &#x27;<strong>latency_max&#x27;</strong> as latency when dataset size is 256 . </p>
                                        <p>And input the latency of the dataset size you want to know as &#x27;<strong>size_pred</strong>&#x27;.</p>
                                    </div>
                                </li>
                            </ol>
                            <p>Finally you can see &quot; Predicted_latency : ——&quot; as a result.</p>
                        </li>
                    </ul>
                    <h5><strong>Profet Validation</strong></h5>
                    <pre class="code">
<code>cd ../validation
python anchor_validaiton.py --anchor_instance g3s.xlarge
python train_scaler.py
python scaler_validation.py --target_instance g3s.xlarge</code>
                    </pre>
                    <p>Finally, we proceed with validation.</p>
                    <ol type="1" class="numbered-list" start="1">
                        <li>Go to validation folder and set anchor_instance parameter when running anchor_validation.py. </li>
                    </ol>
                    <ol type="1" class="numbered-list" start="2">
                        <li>Similarly, for scaler, scaler training is performed with train_scaler.py.  And then, when running scaler_validation.py, target_instance is entered as a parameter to proceed.</li>
                    </ol>
                    <p>As a result, the anchor instance to target instance result is specified and MAPE , R2 , and RMSE are output.</p>
                    <p>I explained how you can use PROFET like this. If you have any further questions, please ask through the Contact section! </p>
                </li>
            </ul>
        </div>
    </div>
<Footer>
</Footer>
</body>
</html>
